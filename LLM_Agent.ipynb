{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Gathnex : https://gathnex.medium.com/how-to-create-your-own-llm-agent-from-scratch-a-step-by-step-guide-14b763e5b3b8"
      ],
      "metadata": {
        "id": "VVs4tvgbt2UC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mprnoX-HDV48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb1ed48-1c0c-4bb9-b219-801217c9d6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q openai langchain py_expression_eval google-api-python-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from langchain.utilities import SerpAPIWrapper, GoogleSearchAPIWrapper\n",
        "from py_expression_eval import Parser\n",
        "from langchain.tools import Tool\n",
        "import re, time, os\n",
        "\n",
        "client = OpenAI(api_key='Openai_api_key')\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = \"Google CSE ID\"\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"Google API Key\""
      ],
      "metadata": {
        "id": "ulP-LzLJcTeI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search = GoogleSearchAPIWrapper()\n",
        "goog = Tool(\n",
        "    name=\"Google Search\",\n",
        "    description=\"Search Google for recent results.\",\n",
        "    func=search.run\n",
        "    )\n",
        "\n",
        "def search(str):\n",
        "    return goog(str)\n",
        "\n",
        "parser = Parser()\n",
        "def calculator(str):\n",
        "    return parser.parse(str).evaluate({})"
      ],
      "metadata": {
        "id": "IY7PmWOuEufy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "System_prompt = \"\"\"\n",
        "Answer the following questions and obey the following commands as best you can.\n",
        "\n",
        "You have access to the following tools:\n",
        "\n",
        "Search: Search: useful for when you need to answer questions about current events. You should ask targeted questions.\n",
        "Calculator: Useful for when you need to answer questions about math. Use python code, eg: 2 + 2\n",
        "Response To Human: When you need to respond to the human you are talking to.\n",
        "\n",
        "You will receive a message from the human, then you should start a loop and do one of two things\n",
        "\n",
        "Option 1: You use a tool to answer the question.\n",
        "For this, you should use the following format:\n",
        "Thought: you should always think about what to do\n",
        "Action: the action to take, should be one of [Search, Calculator]\n",
        "Action Input: \"the input to the action, to be sent to the tool\"\n",
        "\n",
        "After this, the human will respond with an observation, and you will continue.\n",
        "\n",
        "Option 2: You respond to the human.\n",
        "For this, you should use the following format:\n",
        "Action: Response To Human\n",
        "Action Input: \"your response to the human, summarizing what you did and what you learned\"\n",
        "\n",
        "Begin!\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dG4lrONc6-O1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Stream_agent(prompt):\n",
        "    messages = [\n",
        "        { \"role\": \"system\", \"content\": System_prompt },\n",
        "        { \"role\": \"user\", \"content\": prompt },\n",
        "    ]\n",
        "    def extract_action_and_input(text):\n",
        "          action_pattern = r\"Action: (.+?)\\n\"\n",
        "          input_pattern = r\"Action Input: \\\"(.+?)\\\"\"\n",
        "          action = re.findall(action_pattern, text)\n",
        "          action_input = re.findall(input_pattern, text)\n",
        "          return action, action_input\n",
        "    while True:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            temperature=0,\n",
        "            max_tokens=1000,\n",
        "            top_p=1,)\n",
        "        response_text = response.choices[0].message.content\n",
        "        print(response_text)\n",
        "        time.sleep(30)\n",
        "        action, action_input = extract_action_and_input(response_text)\n",
        "        if action[-1] == \"Search\":\n",
        "            tool = search\n",
        "        elif action[-1] == \"Calculator\":\n",
        "            tool = calculator\n",
        "        elif action[-1] == \"Response To Human\":\n",
        "            print(f\"Response: {action_input[-1]}\")\n",
        "            break\n",
        "        observation = tool(action_input[-1])\n",
        "        print(\"Observation: \", observation)\n",
        "        messages.extend([\n",
        "            { \"role\": \"system\", \"content\": response_text },\n",
        "            { \"role\": \"user\", \"content\": f\"Observation: {observation}\" },\n",
        "            ])"
      ],
      "metadata": {
        "id": "wOEdjHWp7dSM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Stream_agent(\"who is current CEO of OpenAI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI3QskSZGmzz",
        "outputId": "af3b6094-c996-4eda-a5ee-6fc3d8bda197"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought: I need to find out who the current CEO of OpenAI is.\n",
            "Action: Search\n",
            "Action Input: \"current CEO of OpenAI\"\n",
            "Observation:  2 days ago ... Mira Murati: Who is the new OpenAI interim CEO? Murati now heads the company ... Now, Musk can say he also used to employ OpenAI's current CEO. ^ Mok, Aaron (September 25, 2023). \"OpenAI CEO Sam Altman says he worked so hard on building his first startup with his ex-boyfriend that he got scurvy\". 2 days ago ... Chief technology officer Mira Murati appointed interim CEO to lead OpenAI; Sam Altman departs the company. CEO and co-founder: Sam Altman, former president of the startup accelerator Y Combinator (2015–2023); President and co-founder: Greg Brockman, former CTO, 3rd ... OpenAI is an AI research and deployment company. Our mission is to ensure that artificial general intelligence benefits all of humanity. Jun 21, 2023 ... You ever watch Star Trek?” Sam Altman, the CEO who has become the most visible face of the current artificial-intelligence boom, has just called ... the quality of airbnb product launches sets the current high water mark in the tech industry: ... i am pro-regulation on frontier systems, which is what openai ... Mar 16, 2023 ... OpenAI CEO Sam Altman says AI will reshape society, acknowledges risks ... This feature is currently only accessible to a small set of users ... 1 day ago ... For years, Mira Murati has worked behind the scenes at OpenAI, overseeing the development and delivery of revolutionary products such as ... Mar 19, 2023 ... Sam Altman, CEO of OpenAI, the co that created ChatGPT, believes that artificial intelligence technology will reshape society as we know it, ...\n",
            "Thought: The search results indicate that Mira Murati is the current interim CEO of OpenAI, taking over from Sam Altman.\n",
            "Action: Response To Human\n",
            "Action Input: \"The current interim CEO of OpenAI is Mira Murati, who took over from Sam Altman.\"\n",
            "Response: The current interim CEO of OpenAI is Mira Murati, who took over from Sam Altman.\n"
          ]
        }
      ]
    }
  ]
}